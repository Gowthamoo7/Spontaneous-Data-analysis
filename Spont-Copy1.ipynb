{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting every slash into new index of sentence\n",
    "a_list = a.apply(lambda x : x.str.split('/'))\n",
    "\n",
    "for i in a_list:\n",
    "    tmp = pd.DataFrame.from_records(a_list[i].tolist()).stack().reset_index(level=1, drop=True).rename('new_{}'.format(i))\n",
    "    a = a.drop(i, axis=1).join(tmp)\n",
    "\n",
    "a = a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_excel(r\"C:\\Users\\Dell\\Desktop\\r&d\\sponttest.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content1']=data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content']=[x for x in data['content'].str.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "import re\n",
    "from pywsd.utils import lemmatize_sentence\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].str.replace(\"/\", \" and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content']= data['content'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment part\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "#nltk vader lexicon \n",
    "final_sentimentt=[]\n",
    "for i in data['content'].values:\n",
    "    try:\n",
    "        senti=SentimentIntensityAnalyzer()\n",
    "        analysis=senti.polarity_scores(i)\n",
    "        final_sentimentt.append(analysis['compound'])\n",
    "    except:\n",
    "        final_sentimentt.append(0)\n",
    "data['final_sentimentt']=final_sentimentt\n",
    "\n",
    "pos=data[['row_index','content','final_sentimentt','rating']][data.final_sentimentt>0]\n",
    "neg=data[['row_index','content','final_sentimentt','rating']][data.final_sentimentt<0]\n",
    "hpos=data[['row_index','content','final_sentimentt','rating']][data.final_sentimentt>0.75]\n",
    "hneg=data[['row_index','content','final_sentimentt','rating']][data.final_sentimentt<-0.25]\n",
    "\n",
    "#Converting the polarity values from continuous to categorical\n",
    "data['final_sentimentt'][data.final_sentimentt==0]= 0\n",
    "data['final_sentimentt'][data.final_sentimentt > 0]= 1\n",
    "data['final_sentimentt'][data.final_sentimentt< 0]= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing to needed format(senti)\n",
    "value = {1.0:'pos',-1.0:'neg',0.0:'neu'} \n",
    "data.final_sentimentt = [value[item] for item in data.final_sentimentt]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places=['dl']\n",
    "#xtracting loc and location \n",
    "pattern='|'.join(places)\n",
    "x=data['content'].str.contains(pattern)\n",
    "data['Sentiment']=x.replace((True,False),('negative','positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].str.replace(\"dl\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aspect extraction part\n",
    "!pip install rake_nltk\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords \n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "import re\n",
    "from pywsd.utils import lemmatize_sentence\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting aspect with higher ranking phrases (top5 min)\n",
    "aspect_adj_string =[]\n",
    "for i in data['content'].values:\n",
    "    try:\n",
    "        r= Rake()\n",
    "        r = Rake(min_length=1, max_length=2)\n",
    "        a=r.extract_keywords_from_text(i)\n",
    "        b=r.get_ranked_phrases()[:10]\n",
    "        aspect_adj_string.append(b)\n",
    "    except:\n",
    "        aspect_adj_string.append(0)\n",
    "data['aspect_adj_string']=aspect_adj_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modelling\n",
    "productquality=['dandruff','hairfall','hair','reduced','dont','buy','increased','conditioner','conditionerror','shiny','soft',\n",
    "                'doesnt','support','wash','scalp','sticky','washing','scalps','oily','dirty','grow','hairs','control','washes',\n",
    "                'smooth','silky','reduces','fall','regrowing','growing','problem','healthy','stopped','variant','decreasing',\n",
    "                'wash','mild','fall','moisturizing','using','conditioner','moisturizer','smoothens','frizzy','hair','manageable',\n",
    "                'gradually','improved','type','results','promising','promise','use','life','exotic','lathers','loss','straight',\n",
    "               'delicate','utilizing','utilize','sleek','dry','tangle','free','satisfied','hairwash','softens','soften','thick',\n",
    "               'rough','hairs','suits','decreased','reduced','completely','reduce','daily','palm','performed','manage','herbal',\n",
    "               'parabeen','antihairfall','baldness','shine','real','feel','healthier','healthy','effects','cures','cure','really',\n",
    "               'working','work','controll','control','stopped','stop','consistency','cooling','refreshing','refresh','slightly',\n",
    "               'last','years','difference','length','doesnt','literally','paraben','natural','protection','protect','relief','sulfate',\n",
    "               'sulphate','improve','texture','feels','feel','quality','expected','product','ayurvedic','shampoos','chemicals','chemical','himalaya','patanjali','nyle','chik','dubious',\n",
    "        'like','shampoo','smelling','bad','good','value','last','ok','never','buy','poor','waste','normal','purchase','didnt','average',\n",
    "        'improvement','nothing','long','time','fake','worthful','worth','almost','gone','setisfied','satisfied','osm','name','opposite',\n",
    "         'products','gud','gd','scent','brand','manufacturer','indian','love','worth','experience','nice','works','tried','disappointed',\n",
    "        'fragrance','best','deal','liked','always','use','super','great','necharal','love','grt','cosmetic','amazing','branded',\n",
    "        'effective','smell','beautiful','thanks','wonderful','assured','fantastic','favorite','excellent','fine','happy','awesome',\n",
    "        'super','fav','useful','recommended','recommended','ok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packaging=['bottle','damaged','pack','leaked','package','packing','sealed','parcel','slippery','wet','cellophane','tap','tape','packaging',\n",
    "          'plaster','piece','tightly','fix','cap','leaking','wasted','leakage','condition','opening','covered','lids','approx','ml','seal',\n",
    "          'damage','open','spilling','lid','defective','spoiled','packet','opened','delivery','packing']\n",
    "\n",
    "pricing=['pricing','price','pric','money','rs','rupees','inr','paisa','cheap','worth','charged','pay','mrp','cost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "data['content'] = data['content'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(query):\n",
    "    s=0\n",
    "    it=0\n",
    "    wl=0\n",
    "         \n",
    "\n",
    "    for x in query: \n",
    "        if x in productquality:        \n",
    "            s +=1                    \n",
    "        if x in packaging:\n",
    "            it +=1                \n",
    "        if x in pricing:\n",
    "            wl +=1\n",
    "        \n",
    "    max_count=max(s,it,wl)\n",
    "    #print(max_count)\n",
    "    if max_count==s:\n",
    "        return 'productquality'\n",
    "    elif max_count==it:\n",
    "        return 'packaging'\n",
    "    elif max_count==wl:\n",
    "        return 'pricing'\n",
    "data['class1']=data['content'].apply(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(query):\n",
    "    s=0\n",
    "    it=0\n",
    "    wl=0\n",
    "         \n",
    "\n",
    "    for x in query: \n",
    "        if x in productquality:        \n",
    "            s +=1                    \n",
    "        if x in packaging:\n",
    "            it +=1                \n",
    "        if x in pricing:\n",
    "            wl +=1\n",
    "        \n",
    "    max_count=max(s,it,wl)\n",
    "    max_count1=max_count-1\n",
    "    if max_count1==s:\n",
    "        return 'productquality'\n",
    "    elif max_count1==it:\n",
    "        return 'packaging'\n",
    "    elif max_count1==wl:\n",
    "        return 'pricing'\n",
    "data['class2']=data['content'].apply(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_excel(r\"C:\\Users\\Dell\\Desktop\\r&d\\sponttest0.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
